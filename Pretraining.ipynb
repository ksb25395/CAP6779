{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pretraining.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM13QfDVoc7mgGMS5GqQLGO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ksb25395/CAP6779/blob/master/Pretraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0B_mp0ipAaTm",
        "colab_type": "text"
      },
      "source": [
        "We continue with previous neural network model for sentiment analysis. We had got a F-Score of 0.617 with a LSTM based deep neural network utilizing a layer of pretrained embeddings. Here, we pretrain the same model for a related task of 5-class fine-grained sentiment analysis. We choose the Stanford Sentiment TreeBank dataset for fine-grained sentiment analysis. \n",
        "\n",
        "We hope that pretraining the final model for fine-grained sentiment analysis will help us get a performance boost in terms of 3-class sentiment analysis. The intuition behind this idea is that by training with a fine-grained-sentiment analysis task, the model learns better sentiment features in the input text, and we can use this pretraining for better twitter sentiment analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_JdWjGDBfb1",
        "colab_type": "code",
        "outputId": "acc17f0a-f7f1-4626-a541-4e294e56decd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STwvPiIjCVsE",
        "colab_type": "code",
        "outputId": "44565da0-3d2a-4466-a30c-02cd9a2ad5ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "EMBEDDING_FILE = '/content/gdrive/My Drive/Colab Notebooks/datasets/Google/GoogleNews-vectors-negative300.bin.gz'\n",
        "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IDAiNUnCfA2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading SST data. We concatenate the train, test and dev datasets\n",
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "path = r'/content/gdrive/My Drive/Colab Notebooks/datasets/SST-5' \n",
        "all_files = glob.glob(path + \"/*.txt\")\n",
        "\n",
        "li = []\n",
        "\n",
        "for filename in all_files:\n",
        "    df = pd.read_csv(filename, sep=\"\\t\", index_col=None, header=None)\n",
        "    li.append(df)\n",
        "\n",
        "sst_df = pd.concat(li, axis=0, ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrW0rPEwFuBb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sst_df.columns = ['sentiment', 'text']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Kjti7HSFwsZ",
        "colab_type": "code",
        "outputId": "91e37d5c-1ced-4223-fbcc-1584093505e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "sst_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>__label__3</td>\n",
              "      <td>Effective but too-tepid biopic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>__label__4</td>\n",
              "      <td>If you sometimes like to go to the movies to h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>__label__5</td>\n",
              "      <td>Emerges as something rare , an issue movie tha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>__label__3</td>\n",
              "      <td>The film provides some great insight into the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>__label__5</td>\n",
              "      <td>Offers that rare combination of entertainment ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    sentiment                                               text\n",
              "0  __label__3                     Effective but too-tepid biopic\n",
              "1  __label__4  If you sometimes like to go to the movies to h...\n",
              "2  __label__5  Emerges as something rare , an issue movie tha...\n",
              "3  __label__3  The film provides some great insight into the ...\n",
              "4  __label__5  Offers that rare combination of entertainment ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xf3HDC7uGMX3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "import string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoHRsJNaGeOL",
        "colab_type": "code",
        "outputId": "c7e968eb-55f6-4506-a572-9b027327dda5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "# preprocessing methods\n",
        "# Removal of URLs\n",
        "def remove_urls(text):\n",
        "  url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "  return url_pattern.sub(r'', text)\n",
        "\n",
        "# Remove @ mentions\n",
        "def remove_mentions(text):\n",
        "  text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text)\n",
        "  return text\n",
        "\n",
        "# strip links\n",
        "def strip_links(text):\n",
        "    link_regex    = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n",
        "    links         = re.findall(link_regex, text)\n",
        "    for link in links:\n",
        "        text = text.replace(link[0], ', ')    \n",
        "    return text\n",
        "\n",
        "# strip entities\n",
        "def strip_all_entities(text):\n",
        "    entity_prefixes = ['@','#']\n",
        "    for separator in  string.punctuation:\n",
        "        if separator not in entity_prefixes :\n",
        "            text = text.replace(separator,' ')\n",
        "    words = []\n",
        "    for word in text.split():\n",
        "        word = word.strip()\n",
        "        if word:\n",
        "            if word[0] not in entity_prefixes:\n",
        "                words.append(word)\n",
        "    return ' '.join(words)\n",
        "\n",
        "# convert emojis\n",
        "!pip install emot\n",
        "from emot.emo_unicode import UNICODE_EMO, EMOTICONS\n",
        "def convert_emoticons(text):\n",
        "  for emot in EMOTICONS:\n",
        "    re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").split()), text)\n",
        "  return text\n",
        "\n",
        "# convert emoji to word\n",
        "!pip install emoji\n",
        "import emoji\n",
        "def convert_emojis(text):\n",
        "    return emoji.demojize(text)\n",
        "\n",
        "# expand contractions\n",
        "!pip install -q contractions\n",
        "import contractions\n",
        "def expand_contractions(text):\n",
        "  return contractions.fix(text)\n",
        "\n",
        "# remove punctuations\n",
        "PUNCT = string.punctuation\n",
        "def remove_punctuation(text):\n",
        "  return text.translate(str.maketrans('', '', PUNCT))\n",
        "\n",
        "# remove stopwords\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "def remove_stopwords(text):\n",
        "  return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
        "\n",
        "# remove frequent words\n",
        "def remove_freqwords(text, FREQ_WORDS):\n",
        "  return \" \".join([word for word in str(text).split() if word not in FREQ_WORDS])\n",
        "\n",
        "# remove rare words\n",
        "def remove_rarewords(text, RAREWORDS):\n",
        "    \"\"\"custom function to remove the rare words\"\"\"\n",
        "    return \" \".join([word for word in str(text).split() if word not in RAREWORDS])\n",
        "\n",
        "# lemmatize words\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
        "def lemmatize_words(text):\n",
        "    pos_tagged_text = nltk.pos_tag(text.split())\n",
        "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
        "\n",
        "# spelling correction"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting emot\n",
            "  Downloading https://files.pythonhosted.org/packages/49/07/20001ade19873de611b7b66a4d5e5aabbf190d65abea337d5deeaa2bc3de/emot-2.1-py3-none-any.whl\n",
            "Installing collected packages: emot\n",
            "Successfully installed emot-2.1\n",
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/8d/521be7f0091fe0f2ae690cc044faf43e3445e0ff33c574eae752dd7e39fa/emoji-0.5.4.tar.gz (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-0.5.4-cp36-none-any.whl size=42175 sha256=66abc5511ad0faf48ae2e337faa2d6c243acec2a01e23b472dcda9cf3599bd64\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/a9/0a/4f8e8cce8074232aba240caca3fade315bb49fac68808d1a9c\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-0.5.4\n",
            "\u001b[K     |████████████████████████████████| 317kB 6.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 245kB 27.3MB/s \n",
            "\u001b[?25h  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2w43m3siKMNV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(df):\n",
        "  # Lower casing\n",
        "  df[\"text\"] = df[\"text\"].str.lower()\n",
        "  df.head()\n",
        "  # remove urls\n",
        "  df[\"text\"] = df[\"text\"].apply(lambda x : remove_urls(x))\n",
        "  # remove mentions\n",
        "  df[\"text\"] = df[\"text\"].apply(lambda x: remove_mentions(x))\n",
        "  # strip links\n",
        "  df[\"text\"] = df[\"text\"].apply(lambda x: strip_links(x))\n",
        "  # strip all entities\n",
        "  df[\"text\"] = df[\"text\"].apply(lambda x: strip_all_entities(x))\n",
        "  # convert all emoticons\n",
        "  df[\"text\"] = df[\"text\"].apply(lambda x : convert_emoticons(x))\n",
        "  # convert all emojis\n",
        "  df[\"text\"] = df[\"text\"].apply(lambda x : convert_emojis(x))\n",
        "  # expand all contractions\n",
        "  df[\"text\"] = df[\"text\"].apply(lambda x : expand_contractions(x))\n",
        "  # remove all punctuations\n",
        "  df[\"text\"] = df[\"text\"].apply(lambda x : remove_punctuation(x))\n",
        "  # remove all stopwords\n",
        "  df[\"text\"] = df[\"text\"].apply(lambda x : remove_stopwords(x))\n",
        "  # remove frequent words\n",
        "  from collections import Counter\n",
        "  cnt = Counter()\n",
        "  for text in df[\"text\"].values:\n",
        "    for word in text.split():\n",
        "      cnt[word] += 1\n",
        "  FREQ_WORDS = set([word for (word, count) in cnt.most_common(10)])\n",
        "  df[\"text\"] = df[\"text\"].apply(lambda x : remove_freqwords(x, FREQ_WORDS))\n",
        "  # remove rare words\n",
        "  n_rare_words = 10\n",
        "  RAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])\n",
        "  df[\"text\"] = df[\"text\"].apply(lambda text: remove_rarewords(text, RAREWORDS))\n",
        "  # lemmatize words\n",
        "  df[\"text\"] = df[\"text\"].apply(lambda text: lemmatize_words(text))\n",
        "  # spelling correction\n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8dVsrSiO9aA",
        "colab_type": "code",
        "outputId": "8dcb354d-f1c6-446b-cdc0-31c54b484a52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "sst_df = preprocess(sst_df)\n",
        "sst_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>__label__3</td>\n",
              "      <td>effective tepid biopic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>__label__4</td>\n",
              "      <td>sometimes go movie fun wasabi place start</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>__label__5</td>\n",
              "      <td>emerges something rare issue honest keenly obs...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>__label__3</td>\n",
              "      <td>provide great insight neurotic mindset comic r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>__label__5</td>\n",
              "      <td>offer rare combination entertainment education</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    sentiment                                               text\n",
              "0  __label__3                             effective tepid biopic\n",
              "1  __label__4          sometimes go movie fun wasabi place start\n",
              "2  __label__5  emerges something rare issue honest keenly obs...\n",
              "3  __label__3  provide great insight neurotic mindset comic r...\n",
              "4  __label__5     offer rare combination entertainment education"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rK89h7mPEcJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sst_df.sentiment = sst_df.sentiment.astype('category')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xg1sa-F69NnW",
        "colab_type": "code",
        "outputId": "6b046b69-a84e-4be0-d5a8-51402585d5f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "dict( enumerate(sst_df['sentiment'].cat.categories ) )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '__label__1',\n",
              " 1: '__label__2',\n",
              " 2: '__label__3',\n",
              " 3: '__label__4',\n",
              " 4: '__label__5'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUTsFFd_98wz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sst_df[\"sentiment\"] = sst_df[\"sentiment\"].cat.codes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g62Y37Tz-2aa",
        "colab_type": "code",
        "outputId": "d8046828-bddd-4c3d-ba7c-6b6b4a63ed2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "sst_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>effective tepid biopic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>sometimes go movie fun wasabi place start</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>emerges something rare issue honest keenly obs...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>provide great insight neurotic mindset comic r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>offer rare combination entertainment education</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment                                               text\n",
              "0          2                             effective tepid biopic\n",
              "1          3          sometimes go movie fun wasabi place start\n",
              "2          4  emerges something rare issue honest keenly obs...\n",
              "3          2  provide great insight neurotic mindset comic r...\n",
              "4          4     offer rare combination entertainment education"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpm8-Jre-7hp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sst_texts = sst_df[\"text\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_pTlEu_Aer3",
        "colab_type": "code",
        "outputId": "bd7d0e73-b672-40ce-e246-aede00b8c671",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        }
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'', lower=True)\n",
        "tokenizer.fit_on_texts(sst_texts)\n",
        "sequences = tokenizer.texts_to_sequences(sst_texts)\n",
        "word_index = tokenizer.word_index\n",
        "print(len(word_index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "14740\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMXqlSO8Ah69",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = pad_sequences(sequences)\n",
        "y_train = to_categorical(list(sst_df[\"sentiment\"]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1qyXeuNEBja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "EMBED_DIM = 300\n",
        "vocab_size = len(word_index) + 1\n",
        "embedding_matrix =np.random.normal(0,np.sqrt(0.25),[vocab_size, EMBED_DIM])\n",
        "\n",
        "count = []\n",
        "for word, i in word_index.items():\n",
        "    if i >= vocab_size:\n",
        "        continue\n",
        "    try:\n",
        "        embedding_vec = word2vec[word]\n",
        "        embedding_matrix[i] = embedding_vec\n",
        "    except KeyError:\n",
        "        count.append(word)\n",
        "        embedding_matrix[i] = np.random.normal(0,np.sqrt(0.25),EMBED_DIM)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsvuc2p8Gj05",
        "colab_type": "code",
        "outputId": "b8a02e01-5ba5-49f5-c0b8-c928255f6ad6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "count\n",
        "len(count)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1554"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LbfoNa_KWqu",
        "colab_type": "code",
        "outputId": "e5677689-5feb-4a67-aef8-90cca3d00187",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "from keras.layers import Embedding\n",
        "embedding_layer = Embedding(vocab_size,\n",
        "                            EMBED_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            trainable=True)\n",
        "seq_len = X_train.shape[1]\n",
        "from keras.layers import Input\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.models import Model\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "inputs = Input(shape=(seq_len,))\n",
        "embeddings = embedding_layer(inputs)\n",
        "# flattened = Flatten()(embeddings)\n",
        "lstm = LSTM(128, dropout=0.5)(embeddings)\n",
        "dense = Dense(16, activation=\"relu\")(lstm)\n",
        "dropout = Dropout(rate=0.5)(dense)\n",
        "output = Dense(y_train.shape[1], activation='softmax')(dropout)\n",
        "model = Model(inputs, output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpn-iexNKf7_",
        "colab_type": "code",
        "outputId": "9c52be66-6338-494c-cf95-1b015d9cb8f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "adam = Adam(lr=1e-3)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=adam,\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfzhEIeuKhnt",
        "colab_type": "code",
        "outputId": "8f2044a6-b354-4231-ea71-e2bd53c72f37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "model.fit(X_train, y_train, batch_size=1000, epochs=20, verbose=1, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/20\n",
            "11855/11855 [==============================] - 15s 1ms/step - loss: 1.6054 - acc: 0.2303\n",
            "Epoch 2/20\n",
            "11855/11855 [==============================] - 14s 1ms/step - loss: 1.5834 - acc: 0.2733\n",
            "Epoch 3/20\n",
            "11855/11855 [==============================] - 13s 1ms/step - loss: 1.5584 - acc: 0.2989\n",
            "Epoch 4/20\n",
            "11855/11855 [==============================] - 13s 1ms/step - loss: 1.5170 - acc: 0.3361\n",
            "Epoch 5/20\n",
            "11855/11855 [==============================] - 14s 1ms/step - loss: 1.4615 - acc: 0.3789\n",
            "Epoch 6/20\n",
            "11855/11855 [==============================] - 13s 1ms/step - loss: 1.4076 - acc: 0.3938\n",
            "Epoch 7/20\n",
            "11855/11855 [==============================] - 13s 1ms/step - loss: 1.3668 - acc: 0.4109\n",
            "Epoch 8/20\n",
            "11855/11855 [==============================] - 13s 1ms/step - loss: 1.3325 - acc: 0.4298\n",
            "Epoch 9/20\n",
            "11855/11855 [==============================] - 13s 1ms/step - loss: 1.2827 - acc: 0.4466\n",
            "Epoch 10/20\n",
            "11855/11855 [==============================] - 13s 1ms/step - loss: 1.2544 - acc: 0.4532\n",
            "Epoch 11/20\n",
            "11855/11855 [==============================] - 13s 1ms/step - loss: 1.2168 - acc: 0.4639\n",
            "Epoch 12/20\n",
            "11855/11855 [==============================] - 13s 1ms/step - loss: 1.1807 - acc: 0.4854\n",
            "Epoch 13/20\n",
            "11855/11855 [==============================] - 13s 1ms/step - loss: 1.1454 - acc: 0.5010\n",
            "Epoch 14/20\n",
            "11855/11855 [==============================] - 13s 1ms/step - loss: 1.0973 - acc: 0.5296\n",
            "Epoch 15/20\n",
            "11855/11855 [==============================] - 13s 1ms/step - loss: 1.0534 - acc: 0.5555\n",
            "Epoch 16/20\n",
            "11855/11855 [==============================] - 13s 1ms/step - loss: 1.0178 - acc: 0.5733\n",
            "Epoch 17/20\n",
            "11855/11855 [==============================] - 13s 1ms/step - loss: 0.9756 - acc: 0.5854\n",
            "Epoch 18/20\n",
            "11855/11855 [==============================] - 13s 1ms/step - loss: 0.9274 - acc: 0.6062\n",
            "Epoch 19/20\n",
            "11855/11855 [==============================] - 13s 1ms/step - loss: 0.8932 - acc: 0.6311\n",
            "Epoch 20/20\n",
            "11855/11855 [==============================] - 14s 1ms/step - loss: 0.8537 - acc: 0.6590\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa87333ad30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5yFAsVpKoxh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('/content/gdrive/My Drive/Colab Notebooks/pretrained.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlLNAsgWNf0B",
        "colab_type": "text"
      },
      "source": [
        "Now we have trained the model with SST-5 dataset. We have saved the model and we can now use it to train a model Twitter Sentiment analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sW5O1JrmNk7P",
        "colab_type": "code",
        "outputId": "fba9d376-4a36-43ad-c01c-b833432d6908",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "semeval_df=pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/datasets/Semeval-2017-4A-English/SemEval2017-task4-dev.subtask-A.english.INPUT.txt', sep=\"\\t\", header=None)\n",
        "semeval_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>619950566786113536</td>\n",
              "      <td>neutral</td>\n",
              "      <td>Picturehouse's, Pink Floyd's, 'Roger Waters: T...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>619969366986235905</td>\n",
              "      <td>neutral</td>\n",
              "      <td>Order Go Set a Watchman in store or through ou...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>619971047195045888</td>\n",
              "      <td>negative</td>\n",
              "      <td>If these runway renovations at the airport pre...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>619974445185302528</td>\n",
              "      <td>neutral</td>\n",
              "      <td>If you could ask an onstage interview question...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>619987808317407232</td>\n",
              "      <td>positive</td>\n",
              "      <td>A portion of book sales from our Harper Lee/Go...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    0  ...    3\n",
              "0  619950566786113536  ...  NaN\n",
              "1  619969366986235905  ...  NaN\n",
              "2  619971047195045888  ...  NaN\n",
              "3  619974445185302528  ...  NaN\n",
              "4  619987808317407232  ...  NaN\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW1xO1pmVL_c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "semeval_df = semeval_df.drop(columns=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYg0A4-zWFvh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "semeval_df.columns = ['id', 'sentiment', 'text']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfg3LtJFW3tI",
        "colab_type": "code",
        "outputId": "a398f509-6917-42fe-d436-f8a351e63798",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "semeval_df = preprocess(semeval_df)\n",
        "semeval_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>619950566786113536</td>\n",
              "      <td>neutral</td>\n",
              "      <td>picturehouse pink floyd roger water walll open...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>619969366986235905</td>\n",
              "      <td>neutral</td>\n",
              "      <td>order go set watchman store website tuesday ge...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>619971047195045888</td>\n",
              "      <td>negative</td>\n",
              "      <td>runway renovation airport prevent see taylor s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>619974445185302528</td>\n",
              "      <td>neutral</td>\n",
              "      <td>could ask onstage interview question miss usa ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>619987808317407232</td>\n",
              "      <td>positive</td>\n",
              "      <td>portion book sale harper lee go set watchman r...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   id  ...                                               text\n",
              "0  619950566786113536  ...  picturehouse pink floyd roger water walll open...\n",
              "1  619969366986235905  ...  order go set watchman store website tuesday ge...\n",
              "2  619971047195045888  ...  runway renovation airport prevent see taylor s...\n",
              "3  619974445185302528  ...  could ask onstage interview question miss usa ...\n",
              "4  619987808317407232  ...  portion book sale harper lee go set watchman r...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hToSor--XZAM",
        "colab_type": "code",
        "outputId": "19c889c5-ab33-4209-9022-0e6cff70bd7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "semeval_df.sentiment = semeval_df.sentiment.astype('category')\n",
        "dict( enumerate(semeval_df['sentiment'].cat.categories ) )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'negative', 1: 'neutral', 2: 'positive'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7GAKEDEX_cG",
        "colab_type": "code",
        "outputId": "03e38bc2-6b8f-4cbe-b6a6-6341b3aa48b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "semeval_df[\"sentiment\"] = semeval_df[\"sentiment\"].cat.codes\n",
        "semeval_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>619950566786113536</td>\n",
              "      <td>1</td>\n",
              "      <td>picturehouse pink floyd roger water walll open...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>619969366986235905</td>\n",
              "      <td>1</td>\n",
              "      <td>order go set watchman store website tuesday ge...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>619971047195045888</td>\n",
              "      <td>0</td>\n",
              "      <td>runway renovation airport prevent see taylor s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>619974445185302528</td>\n",
              "      <td>1</td>\n",
              "      <td>could ask onstage interview question miss usa ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>619987808317407232</td>\n",
              "      <td>2</td>\n",
              "      <td>portion book sale harper lee go set watchman r...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   id  ...                                               text\n",
              "0  619950566786113536  ...  picturehouse pink floyd roger water walll open...\n",
              "1  619969366986235905  ...  order go set watchman store website tuesday ge...\n",
              "2  619971047195045888  ...  runway renovation airport prevent see taylor s...\n",
              "3  619974445185302528  ...  could ask onstage interview question miss usa ...\n",
              "4  619987808317407232  ...  portion book sale harper lee go set watchman r...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E76r2Cj2YKCR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "semeval_texts = semeval_df[\"text\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhHgcehmYRpO",
        "colab_type": "code",
        "outputId": "60adc0b5-8cb9-4c74-ec9d-4bc4ebe47269",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'', lower=True)\n",
        "tokenizer.fit_on_texts(semeval_texts)\n",
        "sequences = tokenizer.texts_to_sequences(semeval_texts)\n",
        "word_index = tokenizer.word_index\n",
        "print(len(word_index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18610\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKFmEpqDZ8PR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = pad_sequences(sequences)\n",
        "y_train = to_categorical(list(semeval_df[\"sentiment\"]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mEmnHwSbrFA",
        "colab_type": "code",
        "outputId": "37b99aa5-8582-41f2-b317-7a4ba90887c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "seq_len_new = X_train.shape[1]\n",
        "print(seq_len_new)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "29\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9K9Se3icazF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMBED_DIM = 300\n",
        "vocab_size = len(word_index) + 1\n",
        "embedding_matrix_new =np.random.normal(0,np.sqrt(0.25),[vocab_size, EMBED_DIM])\n",
        "\n",
        "count = []\n",
        "for word, i in word_index.items():\n",
        "    if i >= vocab_size:\n",
        "        continue\n",
        "    try:\n",
        "        embedding_vec = word2vec[word]\n",
        "        embedding_matrix_new[i] = embedding_vec\n",
        "    except KeyError:\n",
        "        count.append(word)\n",
        "        embedding_matrix_new[i] = np.random.normal(0,np.sqrt(0.25),EMBED_DIM)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FzMVNs0jVTq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_layer = Embedding(vocab_size,\n",
        "                            EMBED_DIM,\n",
        "                            weights=[embedding_matrix_new],\n",
        "                            trainable=True)\n",
        "inputs = Input(shape=(seq_len_new,))\n",
        "embeddings = embedding_layer(inputs)\n",
        "lstm_weights = model.layers[2].get_weights()\n",
        "lstm = LSTM(128, dropout=0.5)(embeddings)\n",
        "dense_weights = model.layers[3].get_weights()\n",
        "dense = Dense(16, activation=\"relu\")(lstm)\n",
        "dropout = Dropout(rate=0.5)(dense)\n",
        "# prev_output_weights = model.layers[5].get_weights()\n",
        "# prev_output = Dense(prev_output_weights[1].shape[0], activation='relu')(dropout)\n",
        "output = Dense(y_train.shape[1], activation='softmax')(dropout)\n",
        "final_model = Model(inputs, output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnHU_HqrmHNd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_model.layers[2].set_weights(lstm_weights)\n",
        "final_model.layers[3].set_weights(dense_weights)\n",
        "# final_model.layers[5].set_weights(prev_output_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWcUYXQLssiV",
        "colab_type": "code",
        "outputId": "95c227f2-64e4-4bfc-da3c-fbe4871b3c59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20632, 29)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxtMTwm5tbNC",
        "colab_type": "code",
        "outputId": "bd9fa214-13e5-4b89-c201-76750729afa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20632, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVyLhQx5tc_2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "adam = Adam(lr=2e-3)\n",
        "\n",
        "final_model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=adam,\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6O7XNShtmou",
        "colab_type": "code",
        "outputId": "1e229f24-e206-485c-d536-0e7cfe208709",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "final_model.fit(X_train, y_train, batch_size=1000, epochs=10, verbose=1, validation_split=0.1, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 18568 samples, validate on 2064 samples\n",
            "Epoch 1/10\n",
            "18568/18568 [==============================] - 25s 1ms/step - loss: 1.0135 - acc: 0.4783 - val_loss: 0.9745 - val_acc: 0.4990\n",
            "Epoch 2/10\n",
            "18568/18568 [==============================] - 23s 1ms/step - loss: 0.8950 - acc: 0.5736 - val_loss: 0.8813 - val_acc: 0.5489\n",
            "Epoch 3/10\n",
            "18568/18568 [==============================] - 23s 1ms/step - loss: 0.8158 - acc: 0.6343 - val_loss: 0.8349 - val_acc: 0.6037\n",
            "Epoch 4/10\n",
            "18568/18568 [==============================] - 22s 1ms/step - loss: 0.7437 - acc: 0.6718 - val_loss: 0.8402 - val_acc: 0.6032\n",
            "Epoch 5/10\n",
            "18568/18568 [==============================] - 22s 1ms/step - loss: 0.6876 - acc: 0.7061 - val_loss: 0.8765 - val_acc: 0.6066\n",
            "Epoch 6/10\n",
            "18568/18568 [==============================] - 22s 1ms/step - loss: 0.6240 - acc: 0.7391 - val_loss: 0.8925 - val_acc: 0.6221\n",
            "Epoch 7/10\n",
            "18568/18568 [==============================] - 23s 1ms/step - loss: 0.5742 - acc: 0.7629 - val_loss: 0.9394 - val_acc: 0.6153\n",
            "Epoch 8/10\n",
            "18568/18568 [==============================] - 23s 1ms/step - loss: 0.5354 - acc: 0.7819 - val_loss: 0.9652 - val_acc: 0.6139\n",
            "Epoch 9/10\n",
            "18568/18568 [==============================] - 22s 1ms/step - loss: 0.4941 - acc: 0.8015 - val_loss: 1.0672 - val_acc: 0.6042\n",
            "Epoch 10/10\n",
            "18568/18568 [==============================] - 22s 1ms/step - loss: 0.4523 - acc: 0.8193 - val_loss: 1.1261 - val_acc: 0.5959\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa86bda8f98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJzN5PiAtumW",
        "colab_type": "code",
        "outputId": "85864089-f7cb-44d4-e5dc-15950a735279",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-0c30c2396633>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msequences_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_set' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCIgK1m4CFE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
